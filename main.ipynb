{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JasonLaux/DeepGlobe/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QCAzZEIaQ0wh"
   },
   "outputs": [],
   "source": [
    "import cv2 # 4.1.2\n",
    "import tensorflow as tf # tf.__version__ = 2.6\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "# Python Version 3.7.12\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72vuK6AoTFye",
    "outputId": "de856918-2ff5-4855-c271-c0a35667623a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Get the path of meta_data.csv.file\n",
    "base_path = os.getcwd()\n",
    "metadata_path = os.path.join(base_path, \"drive/MyDrive/road-extraction/meta_data.csv\")\n",
    "dataset_path = os.path.join(base_path, \"drive/MyDrive/road-extraction/Road_Extraction_Dataset.zip (Unzipped Files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F2-CGBM7rIo6"
   },
   "outputs": [],
   "source": [
    "# Get the local path of meta_data.csv.file\n",
    "base_path = os.getcwd()\n",
    "metadata_path = os.path.join(base_path, \"meta_data.csv\")\n",
    "dataset_path = os.path.join(base_path, \"Road_Extraction_Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFFKbFUbs58z",
    "outputId": "264cd7ee-4400-4b5d-d737-a37c53b6dd9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set:  4180\n",
      "Length of validation set:  1046\n",
      "Length of test set:  1000\n",
      "First row of metadata: \n",
      "    split             sat             mask\n",
      "0  train  100034_sat.jpg  100034_mask.png\n",
      "First 10 rows of training dataset: \n",
      "               sat             mask\n",
      "0  122312_sat.jpg  122312_mask.png\n",
      "1  307151_sat.jpg  307151_mask.png\n",
      "2  212985_sat.jpg  212985_mask.png\n",
      "3  500322_sat.jpg  500322_mask.png\n",
      "4  464724_sat.jpg  464724_mask.png\n",
      "5  425216_sat.jpg  425216_mask.png\n",
      "6  608782_sat.jpg  608782_mask.png\n",
      "7  390893_sat.jpg  390893_mask.png\n",
      "8  210375_sat.jpg  210375_mask.png\n",
      "9  439073_sat.jpg  439073_mask.png\n",
      "First 10 rows of test dataset: \n",
      "               sat             mask\n",
      "0  851461_sat.jpg  851461_mask.png\n",
      "1  851693_sat.jpg  851693_mask.png\n",
      "2  851977_sat.jpg  851977_mask.png\n",
      "3  852056_sat.jpg  852056_mask.png\n",
      "4  852071_sat.jpg  852071_mask.png\n",
      "5   85211_sat.jpg   85211_mask.png\n",
      "6  852321_sat.jpg  852321_mask.png\n",
      "7  852350_sat.jpg  852350_mask.png\n",
      "8  852387_sat.jpg  852387_mask.png\n",
      "9  852441_sat.jpg  852441_mask.png\n"
     ]
    }
   ],
   "source": [
    "meta_data = pd.read_csv(metadata_path, index_col=False)\n",
    "\n",
    "# Get training and test dataset index\n",
    "dataset_index = meta_data[meta_data['split'] == 'train'][['sat', 'mask']]\n",
    "test_dataset_index = meta_data[meta_data['split'] == 'test'][['sat', 'mask']]\n",
    "\n",
    "# Split training dataset into 'train' and 'validation'\n",
    "# Shuffle indices \n",
    "dataset_shuffled = dataset_index.sample(frac=1, ignore_index=True)\n",
    "\n",
    "train_dataset_index = dataset_shuffled[:4180]\n",
    "validate_dataset_index = dataset_shuffled[4180:]\n",
    "\n",
    "# Reset index\n",
    "validate_dataset_index.reset_index(drop=True, inplace=True)\n",
    "test_dataset_index.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Length of training set: \", len(train_dataset_index))\n",
    "print(\"Length of validation set: \", len(validate_dataset_index))\n",
    "print(\"Length of test set: \", len(test_dataset_index))\n",
    "\n",
    "print(\"First row of metadata: \\n\", meta_data.head(1))\n",
    "print(\"First 10 rows of training dataset: \\n\", train_dataset_index.head(10))\n",
    "print(\"First 10 rows of test dataset: \\n\", test_dataset_index.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "id": "N0zWjneM2VIH",
    "outputId": "e5f99241-df2b-4980-800e-4df07e4869b5"
   },
   "outputs": [],
   "source": [
    "# Display the original image and the mask image in the first raw\n",
    "example_sat_path = os.path.join(dataset_path, train_dataset_index.loc[0, 'sat'])\n",
    "example_mask_path = os.path.join(dataset_path, train_dataset_index.loc[0, 'mask'])\n",
    "sat = cv2.imread(example_sat_path)\n",
    "mask = cv2.imread(example_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "mask_bw = cv2.threshold(mask, 128, 255, cv2.THRESH_BINARY)[1]\n",
    "print(\"Original satellite image shape: \", sat.shape)\n",
    "print(\"Grayscale mask image shape: \", mask.shape)\n",
    "print(\"Binary mask image: \\n\", mask_bw)\n",
    "# fig, ax = plt.subplots(3, 1)\n",
    "# ax[0].imshow(cv2.cvtColor(sat, cv2.COLOR_BGR2RGB))\n",
    "# ax[0].set_title(\"Original satellite image\")\n",
    "# ax[1].imshow(cv2.cvtColor(mask, cv2.COLOR_BGR2RGB))\n",
    "# ax[1].set_title(\"Grayscale mask image\")\n",
    "# ax[2].imshow(mask_bw, cmap='gray')\n",
    "# ax[2].set_title(\"Binary mask image(threshold value=128)\")\n",
    "# plt.show()\n",
    "\n",
    "fig1 = plt.figure()\n",
    "plt.imshow(cv2.cvtColor(sat, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original satellite image\")\n",
    "fig2 = plt.figure()\n",
    "plt.imshow(cv2.cvtColor(mask, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Grayscale mask image\")\n",
    "fig3 = plt.figure()\n",
    "plt.imshow(mask_bw, cmap='gray')\n",
    "plt.title(\"Binary mask image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "EsMPFndrNAeg",
    "outputId": "fda7b780-59a1-4413-f061-eafa1c044f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training samples that has been processed: 100  Total:  4180\n",
      "Total number of training samples that has been processed: 200  Total:  4180\n",
      "Total number of training samples that has been processed: 300  Total:  4180\n",
      "Total number of training samples that has been processed: 400  Total:  4180\n",
      "Total number of training samples that has been processed: 500  Total:  4180\n",
      "Total number of training samples that has been processed: 600  Total:  4180\n",
      "Total number of training samples that has been processed: 700  Total:  4180\n",
      "Total number of training samples that has been processed: 800  Total:  4180\n",
      "Total number of training samples that has been processed: 900  Total:  4180\n",
      "Total number of training samples that has been processed: 1000  Total:  4180\n",
      "Total number of training samples that has been processed: 1100  Total:  4180\n",
      "Total number of training samples that has been processed: 1200  Total:  4180\n",
      "Total number of training samples that has been processed: 1300  Total:  4180\n",
      "Total number of training samples that has been processed: 1400  Total:  4180\n",
      "Total number of training samples that has been processed: 1500  Total:  4180\n",
      "Total number of training samples that has been processed: 1600  Total:  4180\n",
      "Total number of training samples that has been processed: 1700  Total:  4180\n",
      "Total number of training samples that has been processed: 1800  Total:  4180\n",
      "Total number of training samples that has been processed: 1900  Total:  4180\n",
      "Total number of training samples that has been processed: 2000  Total:  4180\n",
      "Total number of training samples that has been processed: 2100  Total:  4180\n",
      "Total number of training samples that has been processed: 2200  Total:  4180\n",
      "Total number of training samples that has been processed: 2300  Total:  4180\n",
      "Total number of training samples that has been processed: 2400  Total:  4180\n",
      "Total number of training samples that has been processed: 2500  Total:  4180\n",
      "Total number of training samples that has been processed: 2600  Total:  4180\n",
      "Total number of training samples that has been processed: 2700  Total:  4180\n",
      "Total number of training samples that has been processed: 2800  Total:  4180\n",
      "Total number of training samples that has been processed: 2900  Total:  4180\n",
      "Total number of training samples that has been processed: 3000  Total:  4180\n",
      "Total number of training samples that has been processed: 3100  Total:  4180\n",
      "Total number of training samples that has been processed: 3200  Total:  4180\n",
      "Total number of training samples that has been processed: 3300  Total:  4180\n",
      "Total number of training samples that has been processed: 3400  Total:  4180\n",
      "Total number of training samples that has been processed: 3500  Total:  4180\n",
      "Total number of training samples that has been processed: 3600  Total:  4180\n",
      "Total number of training samples that has been processed: 3700  Total:  4180\n",
      "Total number of training samples that has been processed: 3800  Total:  4180\n",
      "Total number of training samples that has been processed: 3900  Total:  4180\n",
      "Total number of training samples that has been processed: 4000  Total:  4180\n",
      "Total number of training samples that has been processed: 4100  Total:  4180\n",
      "Total number of training samples that has been processed: 100  Total:  1046\n",
      "Total number of training samples that has been processed: 200  Total:  1046\n",
      "Total number of training samples that has been processed: 300  Total:  1046\n",
      "Total number of training samples that has been processed: 400  Total:  1046\n",
      "Total number of training samples that has been processed: 500  Total:  1046\n",
      "Total number of training samples that has been processed: 600  Total:  1046\n",
      "Total number of training samples that has been processed: 700  Total:  1046\n",
      "Total number of training samples that has been processed: 800  Total:  1046\n",
      "Total number of training samples that has been processed: 900  Total:  1046\n",
      "Total number of training samples that has been processed: 1000  Total:  1046\n",
      "Total number of training samples that has been processed: 100  Total:  1000\n",
      "Total number of training samples that has been processed: 200  Total:  1000\n",
      "Total number of training samples that has been processed: 300  Total:  1000\n",
      "Total number of training samples that has been processed: 400  Total:  1000\n",
      "Total number of training samples that has been processed: 500  Total:  1000\n",
      "Total number of training samples that has been processed: 600  Total:  1000\n",
      "Total number of training samples that has been processed: 700  Total:  1000\n",
      "Total number of training samples that has been processed: 800  Total:  1000\n",
      "Total number of training samples that has been processed: 900  Total:  1000\n",
      "Total number of training samples that has been processed: 1000  Total:  1000\n",
      "(4180, 256, 256, 3)\n",
      "(4180, 256, 256, 1)\n",
      "(1046, 256, 256, 3)\n",
      "(1046, 256, 256, 1)\n",
      "(1000, 256, 256, 3)\n",
      "(1000, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "width, height = 256, 256\n",
    "\n",
    "\n",
    "def dataLoader(indexFile, img_width, img_height):\n",
    "    \"\"\"\n",
    "    This function will read image data based on index files.\n",
    "\n",
    "    :param indexFile : the index dataFrame contains image names of satellite and mask image pairs\n",
    "    :param img_width : the width of resized image\n",
    "    :param img_height: the height of resized image\n",
    "    :return      : return satellite and mask image data. General shape: (No. samples, width, height, No. channels)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_samples = len(indexFile)\n",
    "    \n",
    "    input_scale = (img_width, img_height)\n",
    "    \n",
    "    sat = np.empty([num_samples, width, height, 3], dtype = 'float64')\n",
    "    mask = np.empty([num_samples, width, height, 1], dtype = 'float64')\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Read the satellite image and the cooresponding mask image\n",
    "        sat_img = cv2.imread(os.path.join(dataset_path, indexFile.loc[i, 'sat']))\n",
    "        sat_img = sat_img / 255\n",
    "        mask_gray_img = cv2.imread(os.path.join(dataset_path, indexFile.loc[i, 'mask']), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # Binarize mask image. Threshold is 128.\n",
    "        mask_bw = cv2.threshold(mask_gray_img, 128, 1, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "        # Resize images to (width, height)\n",
    "        resized_sat= cv2.resize(sat_img, input_scale, interpolation = cv2.INTER_CUBIC)\n",
    "        resized_mask= cv2.resize(mask_bw, input_scale, interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        # Expand another dim \n",
    "        resized_mask = np.expand_dims(resized_mask, axis=-1)\n",
    "        \n",
    "        sat[i] = resized_sat\n",
    "        mask[i] = resized_mask\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "          print(\"Total number of training samples that has been processed:\", i + 1, \" Total: \", num_samples)\n",
    "    \n",
    "    return sat, mask\n",
    "\n",
    "train_sat, train_mask = dataLoader(train_dataset_index, width, height)\n",
    "val_sat, val_mask = dataLoader(validate_dataset_index, width, height)\n",
    "test_sat, test_mask = dataLoader(test_dataset_index, width, height)\n",
    "\n",
    "\n",
    "print(train_sat.shape)\n",
    "print(train_mask.shape)\n",
    "\n",
    "print(val_sat.shape)\n",
    "print(val_mask.shape)\n",
    "\n",
    "print(test_sat.shape)\n",
    "print(test_mask.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Wiil5ksT7ZpB"
   },
   "outputs": [],
   "source": [
    "class Autoencoder:\n",
    "\n",
    "    def build(width, height, depth, filters=(32, 64, 128)):\n",
    "\n",
    "        # initialize the input shape\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "        # define the input to the encoder\n",
    "        inputs = Input(shape=inputShape)\n",
    "        x = inputs\n",
    "        # construct encoder\n",
    "        for f in filters:\n",
    "            # apply a CONV => RELU => BN operation\n",
    "            x = Conv2D(f, (5, 5), strides=2, padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "        # loop through filters in the reverse order to construct decoder\n",
    "        for f in filters[::-1]:\n",
    "            # apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "            x = Conv2DTranspose(f, (5, 5), strides=2,\n",
    "                padding=\"same\")(x)\n",
    "            x = LeakyReLU(alpha=0.2)(x)\n",
    "            x = BatchNormalization(axis=chanDim)(x)\n",
    "        # apply a single CONV_TRANSPOSE layer used to recover the\n",
    "        # binary mask image\n",
    "        x = Conv2DTranspose(1, (5, 5), padding=\"same\")(x)\n",
    "        outputs = Activation(\"relu\", name=\"decoded\")(x)\n",
    "        # construct our autoencoder model\n",
    "        autoencoder = Model(inputs, outputs, name=\"autoencoder\")\n",
    "        # return the autoencoder model\n",
    "        return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGhjud71Jv6E",
    "outputId": "5346de73-973a-4aa5-8375-f713cc1c6d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 32)      2432      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        51264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 128)       204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 128)       409728    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 128, 128, 64)      204864    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 256, 256, 32)      51232     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 256, 256, 1)       801       \n",
      "_________________________________________________________________\n",
      "decoded (Activation)         (None, 256, 256, 1)       0         \n",
      "=================================================================\n",
      "Total params: 927,041\n",
      "Trainable params: 926,145\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder.build(256, 256, 3)\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUih32xDL8bV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "84/84 [==============================] - 909s 11s/step - loss: 0.7736 - accuracy: 0.9180 - val_loss: 0.5159 - val_accuracy: 0.9583\n",
      "Epoch 2/3\n",
      "84/84 [==============================] - 989s 12s/step - loss: 0.4251 - accuracy: 0.9489 - val_loss: 0.6119 - val_accuracy: 0.9168\n",
      "Epoch 3/3\n",
      " 6/84 [=>............................] - ETA: 11:20 - loss: 0.4377 - accuracy: 0.9502"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "model = autoencoder.fit(train_sat, train_mask, validation_data=(val_sat, val_mask), epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw loss accuracy in the training and validation dataset during training\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.figure()\n",
    "plt.plot(N, model.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, model.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_display = 10\n",
    "pred_mask = autoencoder.predict(train_sat[:num_display])\n",
    "\n",
    "true_mask = train_mask[:num_display]\n",
    "for i in range(num_display):\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(train_sat[i, :, :, :])\n",
    "    fig_sub, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(pred_mask[i])\n",
    "    ax[1].imshow(true_mask[i], cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPPaui+dX0+KTkdqV5y4w0/",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:cv2] *",
   "language": "python",
   "name": "conda-env-cv2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
